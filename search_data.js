const SEARCH_DATA = [
    {
        url: "topics/assumptions_linearity.html",
        title: "Assumptions of Linear Regression",
        content: "Linear Regression isn't just about drawing a line. For the model to be statistically valid and trustworthy, the data must follow a set of 'Rules' (Assumptions). 1. Linear Relationship The Rule: The relationship between the independent variable (X) and the dependent variable (Y) must be linear. Ideally, the data points should arrange themselves in a straight line shape. The Violation: The data shows a curve (parabola, exponential, etc.). If you fit a straight line to curved data, your predictions will be systematically wrong. 2. No Autocorrelation of Error The Rule: The errors (residuals) should be independent of each other. Knowing one error shouldn't help you predict the next one. The Violation: Common in time-series data. If yesterday's error was positive, and today's is also likely positive, you have autocorrelation. Check: Use the Durbin-Watson Test (Values 1.5 - 2.5 are generally normal). 3. Homoscedasticity (Equal Variance) The Rule: The error (noise) should be consistent across all values of X. The Violation (Heteroscedasticity): The error spreads out like a funnel (e.g., predicting house prices becomes harder/noisier for expensive houses). 4. Normality of Residuals The Rule: The errors (residuals) should follow a Normal (Bell Curve) distribution centered at zero. This means most of the error values should be close to zero, with fewer and fewer errors as we move further away from the mean. 5. Multicollinearity The Rule: Independent variables (x1, x2) should NOT be too correlated with each other. If they are, the model gets confused about which variable is actually contributing to the prediction. Detection: We use the VIF (Variance Inflation Factor). VIF = 1: No correlation (Perfect). VIF > 5-10: High multicollinearity (Problematic). The Math: How is VIF calculated? VIF_i = 1 / (1 - R_i^2) Where R_i^2 is the Coefficient of Determination obtained by regressing X_i against all other independent variables."
    },
    {
        url: "topics/regression.html",
        title: "Regression Models (Linear & Polynomial)",
        content: "1. Simple Linear Regression Simple Linear Regression models the relationship between a single independent variable (Input, x) and a dependent variable (Output, y) using a straight line. 2. Polynomial Regression What if the data isn't straight? Polynomial Regression fits a curve by adding powers of x (x^2, x^3...). This is still 'Linear Regression' technically, because it is linear in the parameters (coefficients), even though it produces a curved line. 3. Multiple Linear Regression While Simple/Poly Regression uses one input (x), Multiple Linear Regression uses multiple independent variables (x1, x2, ...). Key Difference In Multiple Regression, we fit a Hyperplane. 4. Handling Categorical Data (Dummy Variables) Linear Regression performs math on numbers. But real data often contains text or categories like 'City', 'Color', or 'Yes/No'. To use these in our model, we must translate them into numbers using Dummy Variables. The Technique: One-Hot Encoding Instead of assigning random numbers (Red=1, Blue=2, Green=3) which would confuse the model into thinking Green > Blue, we create separate 'Switch' columns for each category. Bad Approach (Label Encoding) Error: Model assumes Green is '3x more' than Red. Good Approach (Dummy Vars) Result: Each category is treated independently. Note on the 'Dummy Variable Trap': Technically, if you have 3 categories, you only need 2 dummy variables (e.g., if it's not Red and not Blue, it must be Green). Including all 3 can cause Multicollinearity (Perfect Correlation)."
    },
    {
        url: "topics/parameter_optimization.html",
        title: "Parameter Optimization (OLS & Gradient Descent)",
        content: "In Machine Learning, 'Training' is essentially a search game. We are looking for the 'Magic Numbers' (weights) (and) (biases). 1. The Goal: Defining the Cost Function Before we can 'optimize' (improve) anything, we need to mathematically define what 'bad' looks like. We call this the (Cost) (Function). Step 1: The Prediction (Y') We pick random values for theta (weight) and b (bias). Y' = theta * X + b. Step 2: The Comparison (Error) We compare prediction against actual Y. Error = Y' - Y. Step 3: The Cost (J) We square errors and average them. J(theta) = 1/n sum (Ypred - Yactual)^2. 2. The Solutions: How to Minimize J Method A: Ordinary Least Squares (OLS) The Formula: theta = (XT X)^-1 XT y. Method B: Gradient Descent (The Iterative Walk) Analogy: Blindfolded on a mountain feeling the ground. Learning Rate (alpha): Size of the step. Batch GD, Stochastic GD (SGD), Mini-Batch GD. 3. The Shape of the Cost Function (Convexity) If we have 2 parameters, it becomes a 3D Bowl. Linear Regression is always Convex, meaning Gradient Descent is guaranteed to find the Global Minimum."
    },
    {
        url: "topics/statistical_inference.html",
        title: "Statistical Inference",
        content: "Statistical Inference is the process of using data analysis to deduce properties of an underlying distribution of probability. Essentially, it's about making conclusions about a population based on a sample. 1. Null Hypothesis (H0) & P-Value In statistical hypothesis testing, we use the Null Hypothesis (H0) and the P-Value together to determine if a result is a real effect or just a lucky coincidence. Step 1: The Null Hypothesis (H0) The null hypothesis is NOT a proven fact. It is just our starting stance of skepticism. Think of it like 'Innocent until proven guilty' in a courtroom. We assume the defendant is innocent (H0: No Crime/Effect) not because we have proven they are innocent, but because we need a neutral starting point before looking at the evidence. Example: If you are testing a new drug, we start by assuming 'This drug does nothing' (H0). We hold onto this assumption until the data (evidence) becomes so strong that we are forced to abandon it. Step 2: The P-Value The p-value is a probability (ranging from 0 to 1) that represents how likely it is to observe your specific results if the null hypothesis were actually true. Low P-Value (<= 0.05): Indicates the results are 'strange' or unlikely under the null hypothesis. This gives us evidence to Reject H0. (Statistically Significant). High P-Value (> 0.05): Indicates the results are consistent with chance. We Fail to Reject H0. Step 3: Decision Making We compare the p-value to a pre-set threshold called the Significance Level (alpha), which is most commonly 0.05. If our P-Value is smaller than alpha, we conclude the effect is real! Deep Dive: The Calculation & Logic Step 1: Calculate Test Statistic (Z) = (Observed - Expected) / Error Step 2: P-Value = Area under the curve beyond Z Why Low P = Reject Null? Think of P as the 'Probability of Coincidence'. If P = 0.03 (3%), it means there is only a 3% chance this happened by luck. That's too rare. So we assume it wasn't luck (Reject Null). 2. Confidence Intervals (CI) Instead of a single number estimate, a Confidence Interval gives us a range that is likely to contain the true population parameter. '95% Confidence' means: If we repeated this experiment 100 times, 95 of the calculated intervals would capture the true value. 3. Common Hypothesis Tests While the P-Value is the universal 'score,' we calculate it using different tests depending on the type of data we have. T-Test (Student's t-test) Use when: Comparing the averages (means) of two groups. Example: Does the new website design lead to longer visit times than the old one? Result: P = 0.04 (Significant difference!) ANOVA (Analysis of Variance) Use when: Comparing the averages of three or more groups. Example: Comparing the effectiveness of three different diets. Result: F-Statistic checks if at least one differs from the others. Chi-Square Test Use when: Comparing categories / counts (not averages). Example: Is ice cream flavor preference related to age group? Result: Tests if the distribution of preferences matches expectations."
    },
    {
        url: "topics/simple_regression_example.html",
        title: "Example: Salary vs. Experience",
        content: "Example: Predicting Salary based on Experience Problem Statement A company wants to predict the salary of a new employee based on their years of experience. We have data from 5 current employees. We will use Simple Linear Regression to find the best-fitting line: y = mx + b. 1. The Data Employee Years of Experience (x) Salary in $1000s (y) A 1 30 B 2 35 C 3 50 D 4 60 E 5 75 2. Calculating the Model We need to find the slope (m) and y-intercept (b) that minimizes the error. The formula for slope m is: m = n(sum xy) - (sum x)(sum y) / n(sum x^2) - (sum x)^2 Final Equation Salary = 11.5(Experience) + 15.5 This means for every extra year of experience, the salary increases by $11,500, starting from a base of $15,500. 3. Making a Prediction If we hire someone with 6 years of experience: y = 11.5(6) + 15.5 = 84.5 We would predict a salary of $84,500."
    },
    {
        url: "topics/polynomial_regression_example.html",
        title: "Example: Trajectory of a Ball",
        content: "Example: Trajectory of a Ball Problem Statement We throw a ball into the air and measure its height at different times. We want to predict where the ball will be at 1.5 seconds. Because gravity pulls the ball down, the path is an arc (parabola), not a straight line. Therefore, Simple Linear Regression will fail here. We need Polynomial Regression (Degree 2). 1. The Data Time in Seconds (x) Height in Meters (y) 0 2 1 12 2 12 3 2 2. Choosing the Model A straight line equation looks like y = mx + b. A polynomial (degree 2) equation looks like: y = w0 + w1x + w2x^2 Here, w0, w1, and w2 are the weights we need to find. 3. Fitting the Curve The computer (or a complex manual calculation using matrices) solves for the weights that minimize the error. For this perfect physics example, the result is: w2 = -5 (Gravity's effect) w1 = 15 (Initial upward velocity) w0 = 2 (Starting height) Final Equation Height = -5(Time)^2 + 15(Time) + 2 4. Making a Prediction Let's predict the height at 1.5 seconds (exactly halfway between measurements): y = -5(1.5)^2 + 15(1.5) + 2 y = 13.25 meters The model correctly predicts that the ball peaks between 1 and 2 seconds, reaching higher than the measured points of 12m. A straight line model would have predicted it was just staying flat or going in one direction!"
    }
];