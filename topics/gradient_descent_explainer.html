<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Descent Explained - ML Encyclopedia</title>
    <link rel="stylesheet" href="../style.css">
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        .content-container {
            max-width: 800px;
            margin: 0 auto;
            background: var(--card-bg);
            padding: 3rem;
            border-radius: 16px;
            box-shadow: 0 10px 25px -5px rgba(0, 0, 0, 0.5);
            border: 1px solid var(--border-color);
        }
        
        .back-link {
            display: inline-block;
            margin-bottom: 2rem;
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 600;
        }

        .back-link:hover { text-decoration: underline; }

        h1 { color: var(--text-color); margin-bottom: 1rem; }
        h2 { color: var(--primary-color); border-bottom: 1px solid var(--border-color); padding-bottom: 0.5rem; margin-top: 2rem; }
        h3 { color: #a78bfa; margin-top: 1.5rem; }
        p { color: var(--text-muted); font-size: 1.1rem; line-height: 1.6; }
        
        .highlight {
            color: var(--success-color);
            font-weight: bold;
        }

        .step-box {
            background: rgba(255,255,255,0.03);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
        }

        .formula-box {
            background: #0f172a;
            padding: 1rem;
            border-radius: 8px;
            text-align: center;
            font-size: 1.2rem;
            margin: 1rem 0;
            border: 1px solid #334155;
        }
    </style>
</head>
<body>
    <div class="content-container">
        <a href="parameter_optimization.html" class="back-link">← Back to Parameter Optimization</a>
        
        <h1>Gradient Descent: The Deep Dive</h1>
        
        <p>
            To understand Gradient Descent, imagine you are standing at the top of a mountain (the point of maximum error) and you want to get to the very bottom of the valley (the point of minimum error).
        </p>
        <p>
            <strong>The catch?</strong> You are blindfolded. You can only feel the slope of the ground under your feet to decide which way to move.
        </p>

        <h2>The Step-by-Step Process</h2>

        <div class="step-box">
            <h3>Step 1: Initialize (The Starting Point)</h3>
            <p>
                You start by picking a random value for your parameters, $\theta$ (the weights and bias). At this stage, your model is essentially guessing blindly.
            </p>
            <p>
                <strong>The Result:</strong> Your cost function will likely be very high because the "guess" is probably wrong.
            </p>
        </div>

        <div class="step-box">
            <h3>Step 2: Calculate the Slope (The Gradient)</h3>
            <p>
                You feel the ground around you to see which way is "downhill." Mathematically, this is called taking the <strong>Derivative</strong> of the cost function.
            </p>
            <p>
                The "Gradient" is just a fancy word for the slope.
                <ul>
                    <li>If the slope is positive, you move backward.</li>
                    <li>If the slope is negative, you move forward.</li>
                </ul>
            </p>
        </div>

        <div class="step-box">
            <h3>Step 3: Take a Step (The Learning Rate)</h3>
            <p>
                Once you know the direction, you take a step. But how big should that step be? This is determined by the <strong>Learning Rate ($\alpha$)</strong>.
            </p>
            <ul>
                <li><strong>Small $\alpha$:</strong> You take tiny baby steps. It’s very safe but will take a long time to reach the bottom.</li>
                <li><strong>Large $\alpha$:</strong> You take giant leaps. You might move faster, but you risk overshooting the bottom and "jumping" to the other side of the valley.</li>
            </ul>
        </div>

        <div class="step-box">
            <h3>Step 4: Update the Parameters</h3>
            <p>
                After your step, you update your position. The formula for this update is:
            </p>
            <div class="formula-box">
                $$ \theta_{new} = \theta_{old} - \alpha \cdot (\text{Slope}) $$ 
            </div>
            <p>
                (We subtract because we want to go <em>down</em> the slope, not up).
            </p>
        </div>

        <div class="step-box">
            <h3>Step 5: Repeat (The Iterations)</h3>
            <p>
                You repeat Steps 2 through 4 thousands of times. With every step, your cost function value gets smaller, and your "prediction" gets closer to the "actual" value.
            </p>
        </div>

        <div class="step-box" style="border-color: var(--success-color);">
            <h3 style="color: var(--success-color);">Step 6: Convergence (The Bottom)</h3>
            <p>
                Eventually, the ground becomes flat. The slope becomes zero. No matter which way you move, you aren't going "down" anymore.
            </p>
            <p>
                This is called <strong>Convergence</strong>. You have found the optimal $\theta$!
            </p>
        </div>

    </div>
</body>
</html>